\section{Results}
\subsection{Linear Regression}
\begin{figure}[H]
    \centering
\includegraphics[width = 0.7\paperwidth]{figures/regression_mehtastyle.pdf} 
    \caption{Learned interaction matrix $J_{i,j}$ for the Ising model, for select 
    regularization strengths $\lambda$, generated based on our regression models.
    OLS is not dependent on $Äºambda$, but shown for comparison.
	 } 
\label{fig:regression-mehta}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width = 0.8\paperwidth]{figures/regression_r2.pdf}
    \caption{R2 score performance of the linear regression models as a function of
	     regression parameter $\lambda$.}
\label{fig:regression-r2}
\end{figure}

Figure \ref{fig:regression-r2} shows how the R2 score varies bestween models. 
It's important to note that the $\lambda$ for Ridge regression, and $\alpha$ 
for Lasso regression have the same value, but they affect the models on 
different orders of magnitude, and must be treated somewhat separately. 
Even so, we can see that the training and test set R2-scores
follow eachother closely.

Figure \ref{fig:regression-r2-article} is from article ~\cite{HighBias} and 
shows performance of OLS, Ridge and LASSO regression
on the Ising model as measured by the R2 score.
\subsection{Classifying with Logistic Regression}

\begin{figure}[H]
    \centering
\includegraphics[width = 0.8\textwidth]{figures/logistic_eta.pdf}
    \caption{Accuracies for a selection of learning rates $\eta$ as a function of epochs. 
    30 epochs, batch size $= 100$, and momentum parameter $\gamma = 0.01$. The accuracy
    values are measured on the test set.
    The ratio of amount of training data to test data is $0.8$.}
\label{fig:logistic-eta}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{l|c|c|c}
$\eta$ & Training & Test & Critical  \\
\hline
$10^{-5}$ & $0.718$ & $0.680$ & $0.616$ \\
$10^{-4}$ & $0.723$ & $0.683$ & $0.624$ \\
$10^{-3}$ & $0.723$ & $0.685$ & $0.628$ \\
$10^{-2}$ & $0.712$ & $0.672$ & $0.604$ \\
$10^{-1}$ & $0.462$ & $0.430$ & $0.460$ \\
$1$    & $0.465$ & $0.446$ & $0.480$
\end{tabular}
    \caption{Accuracies for a selection of learning rates $\eta$ after 
    30 epochs. Batch size $= 100$, and momentum parameter $\gamma = 0.01$.
    The ratio of amount of training data to test data is $0.8$}
    \label{tab:logistic-critical}
\end{table}
In table \ref{tab:logistic-critical} the accuracy on data containing critical 
states is included. Due to the varying nature our logistic model, a solution 
in which the weights producing the best fit on test data are stored was 
developed. The accuracies for the critical states are thus not necessarily 
produced with weights as they are after 30 epochs. The reason for this 
descrepancy is yet to be found.

\subsection{Classifying with Neural Network}
\begin{figure}[h]
    \centering
    \subfloat[]{
	\includegraphics[width=0.55\textwidth]{figures/net_acc_test_1.pdf}
	}
    \subfloat[]{
	\includegraphics[width=0.55\textwidth]{figures/net_acc_test_2.pdf}
	}\\
    \subfloat[]{
	\includegraphics[width=0.55\textwidth]{figures/net_acc_crit_1.pdf}
	}
    \subfloat[]{
	\includegraphics[width=0.55\textwidth]{figures/net_acc_crit_2.pdf}
	}
    \caption{
	Neural network classification accuracies for grid search across 
	learning rates $\eta$ and regularization parameters $\lambda$ on 
	the test and critical set after 10 epochs.
	a), c) show the network with one hidden layer, 10 nodes. 
	b), d) show the network with two hidden layers of size 100 (first) and 50
	(second). 
	}
    \label{fig:nn-grids}
\end{figure}
Grid search across multiple orders of magnitude of learning rate and
regularization parameter is shown in figure \ref{fig:nn-grids}. 
Both network architectures were trained on 
$10\%$ of the available samples from the ordered and disordered states.
Comparing the values with those presented in table \ref{tab:logistic-critical},
the network performs much better in classifying the states.
The grid search shows that for the test set the larger architecture with two layers
perform very well on the test set across multiple combinations of parameters,
while the single-layer is more sensitive to the parameter choice.
The performance on the test set is, however, not reflected in the two architectures'
performance on the data from the critial temperature region, as seen in figure 
\ref{fig:nn-grids} (c,d), where the single-layer network actually outperforms the
multilayer network slightly.
