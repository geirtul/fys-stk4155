\section{Discussion}
\subsection{Linear Regression}
\begin{figure}[H]
    \centering
\includegraphics[width = 0.7\paperwidth]{figures/Regression_metha_article.png} 
\caption{Figures from article ~\cite{HighBias} shows similarities to the ones generated by our regression models.} 
\label{fig:regression-mehta-article}
\end{figure}
Figures \ref{fig:regression-mehta} and \ref{fig:regression-mehta-article}
show that both regression models succeed in producing interaction matrices
with most values along the diagonal. 
However, the much larger amount of training data used for our runs 
(the full 10000 samples) give more accurate results, 
as can be seen from the lack of values outside the 
diagonals in the interaction matrices. Reducing the number of samples to 400
produces results of very similar character to those of Mehta et al.

Looking at the R2 scores from figure \ref{fig:regression-r2}, we see 
that the R2 score for the training and test set follow each other closely. 
Again, comparing with articles plot, shown in figure 
\ref{fig:regression-r2-article}, the two have a similar shape 
but there are a larger difference between the R2 score for training 
data and the test data in the article. 
In the articles' figure, the difference seems to be biggest for Ridge and 
OLS. As for our plot, it seems like Ridge has a bit bigger difference 
between the R2 score for training and test data than for the two other 
methods. Again this can likely be attributed to the amount of data regression
was performed on.

\begin{figure}[H]
    \centering
\includegraphics[width = 0.6\paperwidth]{figures/R2_article.png}
\caption{For comparison: Performance of OLS, Ridge and LASSO regression on the 1D Ising model as measured by the 
    R2 score. Figure 16 in \cite{HighBias}} 
\label{fig:regression-r2-article}

\end{figure}
\subsection{Classifying with Logistic Regression}
In figure \ref{fig:logistic-eta}, it is apparent that for some $\eta$ the model 
quickly rises to an approximate highest value, but jumps back down to 
the equivalent of guessing.
When approaching 30 epochs and more, this behaviour 
diminishes somewhat, and overall the etas that produce the best results 
($10^{-5} - 10^{-2}$) have most of their values in the higher points.
From table \ref{tab:logistic-critical} we see that the accuracy is best
for \(\eta = 10^{-3}\). Even so, the accuracy is still 
way below 1.0, making this logistic model only marginally better than guessing. 
Table ~\ref{tab:logistic-critical} can be compared to figure 
~\ref{fig:logistic-article}. We see that our model is slightly 
worse than the one described in the article, but that they 
follow a similar trend.

\begin{figure}[H]
\includegraphics[width = 0.6\paperwidth]{figures/R2_article.png}
\caption{Accuracy as a function of the regularization parameter
\(\lambda\) in classifying the phases of the 2D Ising model on the
training (blue), test (red), and critical (green) data. The solid
and dashed lines compare the ’liblinear’ and ’SGD’ solvers, respectively..Figure 21 in ~\cite{HighBias}} 
\label{fig:logistic-article}
\end{figure}


\subsection{Classifying with Neural Network}
The neural network results in figure \ref{fig:nn-grids} show that the network
performs far better than logistic regression, reaching 1.0 accuracy on both training and
test sets, and as much as 0.97 on the critical set. Adding that the network is trained
on only 10\% of the data that the logistic regressor was applied to, it is significantly better,
and faster.
Note that the network with a more complex architecture, with two layers and far more nodes in 
each layer, doesn't perform better on the data from the critical temperature region. 
This indicates that performance on the test set alone is not a good indicator of how well
the network generalizes. From the grid searches, it also seems that the regularization
parameter plays a larger role for the less complex network.

The fact that the network gains practically nothing in terms of generalization,
when increasing the amount of layers and nodes likely indicates that the problem
is not a very complex one. Thus, two transformations and 10 nodes is all the
network needs in order to quite accurately classify the states.
