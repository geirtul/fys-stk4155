\documentclass[12pt, notitlepage]{article}
%\usepackage[backend=biber]{biblatex}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
%\usepackage{subcaption}
\usepackage{commath}
\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{dirtytalk}
\usepackage{listings}
\usepackage{wasysym}
\usepackage{float}
\usepackage{listings}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{subfig}

% Packages from derivations_fullproblem.tex
\usepackage[squaren]{SIunits}
\usepackage{a4wide}
\usepackage{array}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{titling}

% Parameters for displaying code.
\lstset{language=python}
\lstset{basicstyle=\ttfamily\small}
\lstset{frame=single}
\lstset{keywordstyle=\color{red}\bfseries}
\lstset{commentstyle=\itshape\color{blue}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}

% Define new commands
\newcommand{\expect}[1]{\langle #1 \rangle}
% Add bibliography
\begin{document}


\title{FYS-STK4155 - Project 2}
\author{Geir Tore Ulvik, Idun Kl√∏vstad}
\begin{titlingpage}
\maketitle
\begin{abstract}
In this work four methods to sample or manipulate input data to learning
algorithms are explored: Random over-sampling, SMOTE, ADASYN, and balanced
weighting.
Several way to measure classifier performance are used, and the performance 
of logistic regression and a random forest classifier is evaluated based 
on how they perform on a binary classification problem. 
The data set chosen is payment data from an important bank in Taiwan,
where the data describes credit card holders, and the goal is predicting
whether a customer will default the next payment or not. 
The results presented show that logistic regression can classify the credit 
card data perfectly by applying a balanced weighting of the inputs. 
Random forests reached an accuracy of ~$95\%$ (cross-validation score). 
A possible explanation for the success of weighting the inputs compared 
to the other sampling methods is that some features in the data set may be 
far more crucial in determining the class than others, 
and weighting is the most efficient way of emphasizing these features through 
the learning process. Random forests do not gain similar improvement with
the same weighting, showing most improvement when using random over-sampling.
\end{abstract}
\end{titlingpage}
\include{introduction}
\include{theory}
%\include{method}
\include{results}
\include{discussion}
\include{conclusion}
\appendix
\bibliographystyle{unsrt}
\bibliography{bibliography}
\end{document}
