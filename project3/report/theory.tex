\section{Theory}
\subsection{Imbalanced Data in Classification}
A common challenge in classification is imbalanced data, in which a large
amount of the labeled data belongs to just one or a few of the classes.
For binary classification, if 90\% of the data belongs to one of the classes,
then the classifier is likely to end up placing every single
input in that class, as it will bring its accuracy to 90\%. Techincally, this
accuracy is correct, but it's not very useful since the decision isn't at all
affected by the features of the input. Accuracy alone isn't a good enough
measure of performance to reveal this.

Fortunately, since this is common, a number of methods have been developed
to combat the issue, some of which are described below.

\subsection{Resampling and Weighting}
In resampling there are essentially two main categories: Under-sampling
over-sampling. The difference between them is that over-sampling works with
somehow generating more samples of the minority class, while under-sampling
uses a reduced amount of samples from the majority class.
Weighting the samples is a differente approach in which the samples labeled
as the minority class are weighted higher than the others during training.

\subsubsection{Naive Random Over-sampling}
A very straightforward way to balance a dataset, is to choose random samples 
from the minority class, with replacement, until there is roughly equal
amounts of samples belonging to each class.

\subsubsection{SMOTE}
SMOTE - Synthetic Minority Over-sampling Technique, as the name suggests will
actually synthesize samples from the minority class in order to over-sample,
instead of sampling with replacement. This is done by taking each minority 
class sample and introducing synthetic examples along the line segments joining 
any/all of the k minority class nearest neighbors. Depending upon the amount of 
over-sampling required, neighbors from the k nearest neighbors are randomly
chosen. The result of synthesizing rather than choosing with replacement is
that the decision region is forced to become more general.
See \cite{smote-article} for a more detailed explanation of the methods
involved.

\subsubsection{ADASYN}
Like SMOTE, ADASYN (Adaptive Synthetic Sampling) generates synthetic samples
in order to balance the number of samples in each class. The difference is
mainly that ADASYN uses a density distribution as a criterion to automatically 
decide the number of synthetic samples for each sample in the minority class.
The density distribution is a measurement of the distribution of weight for 
different minority class examples according to their level of difficulty in
learning. This way, ADASYN effectively forces the learning algorithm to 
focus more on examples that are difficult to learn.

\subsubsection{Balanced Weighting}
Scikit-learn's logistic regressor comes with it's own form of handling of
imbalanced data - weighting. It is a streightforward approach in which 
the values of targets are used to to automatically adjust weights inversely 
proportional to class frequencies in the input data as 
$$\frac{samples}{classes \cdot np.bincount(targets)}$$


