\section{Theory}

\subsection{Franke's function}
% Usikker på om den trenger eget underkapittel, men har det sånn
% inntil videre

Franke's function is a weighted sum of four exponentials and has been 
widely used when testing various interpolation and fitting algorithms.
The function is given by 

\begin{align*}
	f(x,y) &= 
	\frac{3}{4}\exp{\left(-\frac{(9x-2)^2}{4} 
	- \frac{(9y-2)^2}{4}\right)}
	+\frac{3}{4}\exp{\left(-\frac{(9x+1)^2}{49}
	- \frac{(9y+1)}{10}\right)} \\
	&+\frac{1}{2}\exp{\left(-\frac{(9x-7)^2}{4} 
	- \frac{(9y-3)^2}{4}\right)} 
	-\frac{1}{5}\exp{\left(-(9x-4)^2 - (9y-7)^2\right) }.
\end{align*}. 

~\cite{Project1}

\subsection{Linear methods for regression}

A linear model assumes that the regression function is linear in the 
inputs \(X_1, \dots, X_p\), and can give a polynomial representation
using basic expansions such as \(X_2 = X_1^2, X_3 = X_1^3\). 

If we have an input vector as described above and want to predict 
a real-valued output \(Y\), the linear model reads as
\begin{equation}
	f(X) = \beta_0 + \sum\limits_{j=1}^p X_j\beta_j
\end{equation}
~\cite{IntroStatistics}

For all three regression methods that we will look at in this report, 
there is a set of training data from which we want to estimate the parameter 
\(\beta\). 

\subsubsection{Ordinary Least Squares method}
The Ordinary Least Squares (OLS) method pick the coefficients
\(\beta = (\beta_0, \beta_1, \dots, \beta_p)^T\) to minimize the 
residual sum of squares given by
\begin{align}
	RSS(\beta) &= \sum\limits_{i=1}^N (y_i - f(x_i))^2\\
				&= \sum\limits_{i=1}^N 
				(y_i - \beta_0 - \sum\limits_{j=1}^p X_{ij}\beta_j )^2\\
\end{align}

This can be rewritten with matrix notation 

\begin{equation}
	RSS(\beta) = (\hat{y}-\vec{X}\beta)^T(\hat{y} - \hat{X}\beta)
\end{equation}. ~\cite{IntroStatistics} 
As this is explained in detail in the book as well as in the 
lecture notes ~\cite{Lectures-notes}, we will not derive
it here.

As we want to minimize the residual sum of squares, we require

\begin{equation}
	\frac{\partial RSS(\hat{\beta})}{\partial \hat{\beta}} 
	= 0 
	= \hat{X}^T(\hat{y} - \hat{X}\hat{\beta})
\end{equation}

This can be rewritten as 

\begin{equation}
	\hat{X}^T\hat{y} = \hat{X}^T\hat{X}\hat{\beta} 
\end{equation}.

We now has to assume that the matrix \(\hat{X}^T\hat{X}\) 
is invertible to get the solution

\begin{equation}
	\hat{\beta} = (\hat{X}^T\hat{X})^{-1}\hat{X}^T\hat{y}
\end{equation}
~\cite{Lectures-notes}~\cite{IntroStatistics}

\subsubsection{Ridge regression}
The OLS will not have a solution if the design matrix \(\hat{X}\) is singular
or near singular, as it depends in \(\hat{X}^T\hat{X}\) being invetertible. 
~\cite{Lectures-notes}

Ridge regression is known as a shrinkage method. It shrinks the regression
coefficients \(\hat{\beta}\) by imposing a penalty on their size
~\cite{IntroStatistics}. We are in other words adding a diagonal component
to the matrix to invert. From OLS we therefore make the following change

\begin{equation}
	\hat{X}^T\hat{X} \rightarrow \hat{X}^T\hat{X} + \lambda \hat{I}
\end{equation}

where I is the identity matrix.~\cite{Lectures-notes}.
Thus we get 

\begin{equation}
	RSS(\lambda) = (\hat{y}-\hat{X}\hat{\beta)}^T(\hat{y} 
	- \hat{X}\hat{\beta})
	- \lambda \hat{\beta}^T\hat{\beta}
\end{equation}
and
\begin{equation}
	\hat{\beta}^{ridge} = (\hat{X}^T\hat{X} 
	+ \lambda \hat{I})^{-1}\hat{X}^T\hat{y} 
\end{equation}
\cite{IntroStatistics}

\subsubsection{Lasso regression}








