\section{Implementation of methods}
All code can be found on github, using the following link:\\ 
https://github.com/geirtul/fys-stk4155/tree/master/project1/src\\

There is one class for each regression method, with corresponding 
filenames. Mean Squared Error and R2 Scored are implemented as class methods.

In all the methods, Franke's Function is defined for 
\(x,y \ \epsilon \ [0,1]\).

\subsection{Ordinary Least Squares}
The class ols.py is made to make a prediction of Franke's function
based on the OLS method. 

The following lines calculates \(\hat{\beta}\) as described in 
section ~\ref{sec:ols}, opting for using Numpy's pseudo-inverse in case
the input is a sparse matrix (non-invertible).

\lstinputlisting[firstline=31,lastline=31]{../src/ols.py}

Then the following lines makes a predicted outcome:

\lstinputlisting[firstline=41,lastline=41]{../src/ols.py}

\subsection{Ridge}
As stated in section ~\ref{seq:ridge}, there is 
not a huge difference from OLS to ridge regression. 

Ridge regression to estimate the parameters \(\hat{\beta}\)
was implemented like this:

\lstinputlisting[firstline=37,lastline=37]{../src/ridge.py}.

As we only change \(\beta\), the code for finding the estimated function 
is the same, only with the new beta. 

\subsection{Lasso}
We opted for using scikit learn's implementation of lasso regression, but placed
into the same framework and class structure as the other methods presented here.

\subsection{Bootstrapping} 
We chose to use bootstrap because of a number of advantages listed in the 
lecture notes. ~\cite{LectureNotes-FysStk}

The data is first split into training and test sets. 
Then the bootstrap method described in ~\ref{seq:bootstrap} is 
implemented as follows.

\lstinputlisting[firstline=34,lastline=75]{../src/resampling.py}.
~\cite{BiasAndVariance}

